{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! tar -xvjf data.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sequences\n",
       "0  MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...\n",
       "1  MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...\n",
       "2  MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...\n",
       "3  MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...\n",
       "4  MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df = pd.read_table('data/family_classification_sequences.tab')\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_codones(sseq):\n",
    "    crop = len(sseq) % 3\n",
    "    cropped_seq = sseq[:-crop] if crop > 0 else sseq\n",
    "\n",
    "    return [cropped_seq[i:i+3] for i in range(0, len(cropped_seq), 3)]\n",
    "\n",
    "def seq_to3(seq):\n",
    "    splittings = [make_codones(seq[i:]) for i in range(3)]\n",
    "    return splittings\n",
    "\n",
    "def create_all_codones(df):\n",
    "    codones = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :][0]\n",
    "        codones.extend(seq_to3(row))\n",
    "    return codones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create(read_path, producer):\n",
    "    if os.path.isfile(read_path):\n",
    "        print('reading', read_path)\n",
    "        with open(read_path, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "    result = producer()\n",
    "    print('saving', read_path)\n",
    "    with open(read_path, 'wb') as fp:\n",
    "        pickle.dump(result, fp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data/all_codones.pickle\n"
     ]
    }
   ],
   "source": [
    "all_codones = read_or_create(read_path='data/all_codones.pickle',\n",
    "                             producer= lambda: create_all_codones(seq_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MAF', 'SAE', 'DVL', 'KEY', 'DRR', 'RRM', 'EAL', 'LLS', 'LYY', 'PND', 'RKL', 'LDY', 'KEW', 'SPP', 'RVQ', 'VEC', 'PKA', 'PVE', 'WNN', 'PPS', 'EKG', 'LIV', 'GHF', 'SGI', 'KYK', 'GEK', 'AQA', 'SEV', 'DVN', 'KMC', 'CWV', 'SKF', 'KDA', 'MRR', 'YQG', 'IQT', 'CKI', 'PGK', 'VLS', 'DLD', 'AKI', 'KAY', 'NLT', 'VEG', 'VEG', 'FVR', 'YSR', 'VTK', 'QHV', 'AAF', 'LKE', 'LRH', 'SKQ', 'YEN', 'VNL', 'IHY', 'ILT', 'DKR', 'VDI', 'QHL', 'EKD', 'LVK', 'DFK', 'ALV', 'ESA', 'HRM', 'RQG', 'HMI', 'NVK', 'YIL', 'YQL', 'LKK', 'HGH', 'GPD', 'GPD', 'ILT', 'VKT', 'GSK', 'GVL', 'YDD', 'SFR', 'KIY', 'TDL', 'GWK', 'FTP'], ['AFS', 'AED', 'VLK', 'EYD', 'RRR', 'RME', 'ALL', 'LSL', 'YYP', 'NDR', 'KLL', 'DYK', 'EWS', 'PPR', 'VQV', 'ECP', 'KAP', 'VEW', 'NNP', 'PSE', 'KGL', 'IVG', 'HFS', 'GIK', 'YKG', 'EKA', 'QAS', 'EVD', 'VNK', 'MCC', 'WVS', 'KFK', 'DAM', 'RRY', 'QGI', 'QTC', 'KIP', 'GKV', 'LSD', 'LDA', 'KIK', 'AYN', 'LTV', 'EGV', 'EGF', 'VRY', 'SRV', 'TKQ', 'HVA', 'AFL', 'KEL', 'RHS', 'KQY', 'ENV', 'NLI', 'HYI', 'LTD', 'KRV', 'DIQ', 'HLE', 'KDL', 'VKD', 'FKA', 'LVE', 'SAH', 'RMR', 'QGH', 'MIN', 'VKY', 'ILY', 'QLL', 'KKH', 'GHG', 'PDG', 'PDI', 'LTV', 'KTG', 'SKG', 'VLY', 'DDS', 'FRK', 'IYT', 'DLG', 'WKF', 'TPL'], ['FSA', 'EDV', 'LKE', 'YDR', 'RRR', 'MEA', 'LLL', 'SLY', 'YPN', 'DRK', 'LLD', 'YKE', 'WSP', 'PRV', 'QVE', 'CPK', 'APV', 'EWN', 'NPP', 'SEK', 'GLI', 'VGH', 'FSG', 'IKY', 'KGE', 'KAQ', 'ASE', 'VDV', 'NKM', 'CCW', 'VSK', 'FKD', 'AMR', 'RYQ', 'GIQ', 'TCK', 'IPG', 'KVL', 'SDL', 'DAK', 'IKA', 'YNL', 'TVE', 'GVE', 'GFV', 'RYS', 'RVT', 'KQH', 'VAA', 'FLK', 'ELR', 'HSK', 'QYE', 'NVN', 'LIH', 'YIL', 'TDK', 'RVD', 'IQH', 'LEK', 'DLV', 'KDF', 'KAL', 'VES', 'AHR', 'MRQ', 'GHM', 'INV', 'KYI', 'LYQ', 'LLK', 'KHG', 'HGP', 'DGP', 'DIL', 'TVK', 'TGS', 'KGV', 'LYD', 'DSF', 'RKI', 'YTD', 'LGW', 'KFT'], ['MSI', 'IGA', 'TRL', 'QND', 'KSD', 'TYS', 'AGP', 'CYA', 'GGC', 'SAF', 'TPR', 'GTC', 'GKD', 'WDL', 'GEQ', 'TCA', 'SGF', 'CTS', 'QPL', 'CAR', 'IKK', 'TQV', 'CGL', 'RYS', 'SKG', 'KDP', 'LVS', 'AEW', 'DSR', 'GAP', 'YVR', 'CTY', 'DAD', 'LID', 'TQA', 'QVD', 'QFV', 'SMF', 'GES', 'PSL', 'AER', 'YCM', 'RGV', 'KNT', 'AGE', 'LVS', 'RVS', 'SDA', 'DPA', 'GGW', 'CRK', 'WYS', 'AHR', 'GPD', 'QDA', 'ALG', 'SFC', 'IKN', 'PGA', 'ADC', 'KCI', 'NRA', 'SDP', 'VYQ', 'KVK', 'TLH', 'AYP', 'DQC', 'WYV', 'PCA', 'ADV', 'GEL', 'KMG', 'TQR', 'DTP', 'TNC', 'PTQ', 'VCQ', 'IVF', 'NML', 'DDG', 'SVT', 'MDD', 'VKN', 'TIN', 'CDF', 'SKY', 'VPP', 'PPP', 'PKP', 'TPP', 'TPP', 'TPP', 'TPP', 'TPP', 'TPP', 'TPP', 'TPR', 'PVH', 'NRK', 'VMF', 'FVA', 'GAV', 'LVA', 'ILI', 'STV'], ['SII', 'GAT', 'RLQ', 'NDK', 'SDT', 'YSA', 'GPC', 'YAG', 'GCS', 'AFT', 'PRG', 'TCG', 'KDW', 'DLG', 'EQT', 'CAS', 'GFC', 'TSQ', 'PLC', 'ARI', 'KKT', 'QVC', 'GLR', 'YSS', 'KGK', 'DPL', 'VSA', 'EWD', 'SRG', 'APY', 'VRC', 'TYD', 'ADL', 'IDT', 'QAQ', 'VDQ', 'FVS', 'MFG', 'ESP', 'SLA', 'ERY', 'CMR', 'GVK', 'NTA', 'GEL', 'VSR', 'VSS', 'DAD', 'PAG', 'GWC', 'RKW', 'YSA', 'HRG', 'PDQ', 'DAA', 'LGS', 'FCI', 'KNP', 'GAA', 'DCK', 'CIN', 'RAS', 'DPV', 'YQK', 'VKT', 'LHA', 'YPD', 'QCW', 'YVP', 'CAA', 'DVG', 'ELK', 'MGT', 'QRD', 'TPT', 'NCP', 'TQV', 'CQI', 'VFN', 'MLD', 'DGS', 'VTM', 'DDV', 'KNT', 'INC', 'DFS', 'KYV', 'PPP', 'PPP', 'KPT', 'PPT', 'PPT', 'PPT', 'PPT', 'PPT', 'PPT', 'PPT', 'PRP', 'VHN', 'RKV', 'MFF', 'VAG', 'AVL', 'VAI', 'LIS', 'TVR'], ['IIG', 'ATR', 'LQN', 'DKS', 'DTY', 'SAG', 'PCY', 'AGG', 'CSA', 'FTP', 'RGT', 'CGK', 'DWD', 'LGE', 'QTC', 'ASG', 'FCT', 'SQP', 'LCA', 'RIK', 'KTQ', 'VCG', 'LRY', 'SSK', 'GKD', 'PLV', 'SAE', 'WDS', 'RGA', 'PYV', 'RCT', 'YDA', 'DLI', 'DTQ', 'AQV', 'DQF', 'VSM', 'FGE', 'SPS', 'LAE', 'RYC', 'MRG', 'VKN', 'TAG', 'ELV', 'SRV', 'SSD', 'ADP', 'AGG', 'WCR', 'KWY', 'SAH', 'RGP', 'DQD', 'AAL', 'GSF', 'CIK', 'NPG', 'AAD', 'CKC', 'INR', 'ASD', 'PVY', 'QKV', 'KTL', 'HAY', 'PDQ', 'CWY', 'VPC', 'AAD', 'VGE', 'LKM', 'GTQ', 'RDT', 'PTN', 'CPT', 'QVC', 'QIV', 'FNM', 'LDD', 'GSV', 'TMD', 'DVK', 'NTI', 'NCD', 'FSK', 'YVP', 'PPP', 'PPK', 'PTP', 'PTP', 'PTP', 'PTP', 'PTP', 'PTP', 'PTP', 'PTP', 'RPV', 'HNR', 'KVM', 'FFV', 'AGA', 'VLV', 'AIL', 'IST', 'VRW'], ['MQN', 'PLP', 'EVM', 'SPE', 'HDK', 'RTT', 'TPM', 'SKE', 'ANK', 'FIR', 'ELD', 'KKP', 'GDL', 'AVV', 'SDF', 'VKR', 'NTG', 'KRL', 'PIG', 'KRS', 'NLY', 'VRI', 'CDL', 'SGT', 'IYM', 'GET', 'FIL', 'ESW', 'EEL', 'YLP', 'EPT', 'KME', 'VLG', 'TLE', 'SCC', 'GIP', 'PFP', 'EWI', 'VMV', 'GED', 'QCV', 'YAY', 'GDE', 'EIL', 'LFA', 'YSV', 'KQL', 'VEE', 'GIQ', 'ETG', 'ISY', 'KYP', 'DDI', 'SDV', 'DEE', 'VLQ', 'QDE', 'EIQ', 'KIR', 'KKT', 'REF', 'VDK', 'DAQ', 'EFQ', 'DFL', 'NSL', 'DAS', 'LLS'], ['QNP', 'LPE', 'VMS', 'PEH', 'DKR', 'TTT', 'PMS', 'KEA', 'NKF', 'IRE', 'LDK', 'KPG', 'DLA', 'VVS', 'DFV', 'KRN', 'TGK', 'RLP', 'IGK', 'RSN', 'LYV', 'RIC', 'DLS', 'GTI', 'YMG', 'ETF', 'ILE', 'SWE', 'ELY', 'LPE', 'PTK', 'MEV', 'LGT', 'LES', 'CCG', 'IPP', 'FPE', 'WIV', 'MVG', 'EDQ', 'CVY', 'AYG', 'DEE', 'ILL', 'FAY', 'SVK', 'QLV', 'EEG', 'IQE', 'TGI', 'SYK', 'YPD', 'DIS', 'DVD', 'EEV', 'LQQ', 'DEE', 'IQK', 'IRK', 'KTR', 'EFV', 'DKD', 'AQE', 'FQD', 'FLN', 'SLD', 'ASL'], ['NPL', 'PEV', 'MSP', 'EHD', 'KRT', 'TTP', 'MSK', 'EAN', 'KFI', 'REL', 'DKK', 'PGD', 'LAV', 'VSD', 'FVK', 'RNT', 'GKR', 'LPI', 'GKR', 'SNL', 'YVR', 'ICD', 'LSG', 'TIY', 'MGE', 'TFI', 'LES', 'WEE', 'LYL', 'PEP', 'TKM', 'EVL', 'GTL', 'ESC', 'CGI', 'PPF', 'PEW', 'IVM', 'VGE', 'DQC', 'VYA', 'YGD', 'EEI', 'LLF', 'AYS', 'VKQ', 'LVE', 'EGI', 'QET', 'GIS', 'YKY', 'PDD', 'ISD', 'VDE', 'EVL', 'QQD', 'EEI', 'QKI', 'RKK', 'TRE', 'FVD', 'KDA', 'QEF', 'QDF', 'LNS', 'LDA', 'SLL'], ['MDS', 'LNE', 'VCY', 'EQI', 'KGT', 'FYK', 'GLF', 'GDF', 'PLI', 'VDK', 'KTG', 'CFN', 'ATK', 'LCV', 'LGG', 'KRF', 'VDW', 'NKT', 'LRS', 'KKL', 'IQY', 'YET', 'RCD', 'IKT', 'ESL', 'LYE', 'IKG', 'DNN', 'DEI', 'TKQ', 'ITG', 'TYL', 'PKE', 'FIL', 'DIA', 'SWI', 'SVE', 'FYD', 'KCN', 'NII', 'INY', 'FVN', 'EYK', 'TMD', 'KKT', 'LQS', 'KIN', 'EVE', 'EKM', 'QKL', 'LNE', 'KEE', 'ELQ', 'EKN', 'DKI', 'DEL', 'ILF', 'SKR', 'MEE', 'DRK', 'KDR', 'EMM', 'IKQ', 'EKM', 'LRE', 'LGI', 'HLE', 'DVS', 'SQN', 'NEL', 'IEK', 'VDE', 'QVE', 'QNA', 'VLN', 'FKI', 'DNI', 'QNK', 'LEI', 'AVE', 'DRA', 'PQP', 'KQN', 'LKR', 'ERF', 'ILL', 'KRN', 'DDY', 'YPY', 'YTI', 'RAQ', 'DIN', 'ARS', 'ALK', 'RQK', 'NLY', 'NEV', 'SVL', 'LDL', 'TCH', 'PNS', 'KTL', 'YVR', 'VKD', 'ELK', 'QKG', 'VVF', 'NLC', 'KVS', 'ISN', 'SKI', 'NEE', 'ELI', 'KAM', 'ETI', 'NDE', 'KRD']]\n"
     ]
    }
   ],
   "source": [
    "print(all_codones[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(index_words_list, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index_words in index_words_list:\n",
    "        for index, center in enumerate(index_words):\n",
    "            context = random.randint(1, context_window_size)\n",
    "            # get a random target before the center word\n",
    "            for target in index_words[max(0, index - context): index]:\n",
    "                yield center, target\n",
    "            # get a random target after the center wrod\n",
    "            for target in index_words[index + 1: index + context + 1]:\n",
    "                yield center, target\n",
    "\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "\n",
    "glob_prob = None\n",
    "def cod_to_dict(cod, dictionary):\n",
    "    return [dictionary[key] for key in cod if random.random() >= glob_prob[key]]\n",
    "\n",
    "def make_dictionary(all_codones):\n",
    "    global glob_prob\n",
    "    \n",
    "    flat_codones = flatten(all_codones)\n",
    "    counter = collections.Counter(flat_codones)\n",
    "    print(counter.most_common(5))\n",
    "    unique_codones = set(flat_codones)\n",
    "    \n",
    "    # Subsampling\n",
    "    glob_prob = dict()\n",
    "    sample_rate = 1e-5\n",
    "    num_total_codones = len(flat_codones)\n",
    "    for codone in unique_codones:\n",
    "        # https://github.com/chrisjmccormick/word2vec_commented/blob/07e9576bbf05c9cefaf8aa590555ce9bc04b041b/word2vec.c#L740\n",
    "        freq = float(counter[codone]) / num_total_codones\n",
    "        ran = 1 - math.sqrt(sample_rate / freq)\n",
    "        glob_prob[codone] = ran\n",
    "    \n",
    "    lst_sorted = sorted(unique_codones, key=counter.get, reverse=True)\n",
    "    for i in range(5):\n",
    "        print(lst_sorted[i], glob_prob[lst_sorted[i]])\n",
    "    for i in range(-5, -1):\n",
    "        print(lst_sorted[i], glob_prob[lst_sorted[i]])\n",
    "    print(lst_sorted[:5])\n",
    "    dictionary = {cod: i for i, cod in enumerate(lst_sorted)}\n",
    "    return dictionary\n",
    "\n",
    "def process_data(all_codones, dictionary, batch_size, skip_window):\n",
    "    cod_dicts = [cod_to_dict(cod, dictionary) for cod in all_codones]\n",
    "    single_gen = generate_sample(cod_dicts, context_window_size=skip_window)\n",
    "    batch_gen = get_batch(single_gen, batch_size=batch_size)\n",
    "    return batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAA', 152576), ('LLL', 131965), ('ALA', 127540), ('LAA', 126274), ('AAL', 124613)]\n",
      "AAA 0.905112767968274\n",
      "LLL 0.8979714982063004\n",
      "ALA 0.896216650220804\n",
      "LAA 0.8956976912938254\n",
      "AAL 0.8950048557166559\n",
      "GPX -36.06388080058536\n",
      "AZL -36.06388080058536\n",
      "XMT -36.06388080058536\n",
      "XVH -36.06388080058536\n",
      "['AAA', 'LLL', 'ALA', 'LAA', 'AAL']\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_dictionary(all_codones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SKIP_WINDOW = 12  # the context window\n",
    "\n",
    "batch_gen = process_data(all_codones, dictionary, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.saver = tf.train.Saver()  # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        with tf.name_scope(\"embed\"):\n",
    "            self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size,\n",
    "                                                               self.embed_size], -1.0, 1.0),\n",
    "                                            name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                         stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "                                     name='nce_weight')\n",
    "            nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                      biases=nce_bias,\n",
    "                                                      labels=self.target_words,\n",
    "                                                      inputs=embed,\n",
    "                                                      num_sampled=self.num_sampled,\n",
    "                                                      num_classes=self.vocab_size), name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss,\n",
    "                                                                                 global_step=self.global_step)\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 9424\n",
    "EMBED_SIZE = 100  # dimension of the word embedding vectors\n",
    "NUM_SAMPLED = 25  # Number of negative examples to sample.\n",
    "LEARNING_RATE = .9\n",
    "NUM_TRAIN_STEPS = 400000\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_gen, num_train_steps, learning_rate, skip_step):\n",
    "    make_dir('checkpoints')\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    with tf.Session(graph=g, config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state('checkpoints')\n",
    "#         if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and os.path.isfile(ckpt.model_checkpoint_path + '.meta'):\n",
    "            print('restored checkpoint')\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0  # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(learning_rate), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "            feed_dict = {model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op],\n",
    "                                              feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "                total_loss = 0.0\n",
    "                model.saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "        final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        return final_embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored checkpoint\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/skip-gram-603999\n",
      "Average loss at step 605999:  35.3\n",
      "Average loss at step 607999:  17.0\n",
      "Average loss at step 609999:  12.7\n",
      "Average loss at step 611999:   8.3\n",
      "Average loss at step 613999:   6.3\n",
      "Average loss at step 615999:   5.4\n",
      "Average loss at step 617999:   4.5\n",
      "Average loss at step 619999:   3.8\n",
      "Average loss at step 621999:   4.5\n",
      "Average loss at step 623999:   4.3\n",
      "Average loss at step 625999:   4.2\n",
      "Average loss at step 627999:   3.9\n",
      "Average loss at step 629999:   3.9\n",
      "Average loss at step 631999:   3.9\n",
      "Average loss at step 633999:   3.4\n",
      "Average loss at step 635999:   3.5\n",
      "Average loss at step 637999:   3.3\n",
      "Average loss at step 639999:   3.6\n",
      "Average loss at step 641999:   3.4\n",
      "Average loss at step 643999:   3.3\n",
      "Average loss at step 645999:   3.6\n",
      "Average loss at step 647999:   3.3\n",
      "Average loss at step 649999:   3.7\n",
      "Average loss at step 651999:   3.6\n",
      "Average loss at step 653999:   3.4\n",
      "Average loss at step 655999:   2.9\n",
      "Average loss at step 657999:   3.2\n",
      "Average loss at step 659999:   2.3\n",
      "Average loss at step 661999:   3.0\n",
      "Average loss at step 663999:   3.5\n",
      "Average loss at step 665999:   3.4\n",
      "Average loss at step 667999:   3.6\n",
      "Average loss at step 669999:   3.5\n",
      "Average loss at step 671999:   3.6\n",
      "Average loss at step 673999:   3.6\n",
      "Average loss at step 675999:   3.5\n",
      "Average loss at step 677999:   3.2\n",
      "Average loss at step 679999:   3.5\n",
      "Average loss at step 681999:   3.4\n",
      "Average loss at step 683999:   3.6\n",
      "Average loss at step 685999:   3.5\n",
      "Average loss at step 687999:   3.5\n",
      "Average loss at step 689999:   3.5\n",
      "Average loss at step 691999:   3.4\n",
      "Average loss at step 693999:   3.5\n",
      "Average loss at step 695999:   3.4\n",
      "Average loss at step 697999:   3.5\n",
      "Average loss at step 699999:   3.5\n",
      "Average loss at step 701999:   3.3\n",
      "Average loss at step 703999:   3.4\n",
      "Average loss at step 705999:   3.5\n",
      "Average loss at step 707999:   3.4\n",
      "Average loss at step 709999:   3.4\n",
      "Average loss at step 711999:   3.4\n",
      "Average loss at step 713999:   3.4\n",
      "Average loss at step 715999:   3.4\n",
      "Average loss at step 717999:   2.9\n",
      "Average loss at step 719999:   3.3\n",
      "Average loss at step 721999:   3.3\n",
      "Average loss at step 723999:   3.0\n",
      "Average loss at step 725999:   3.2\n",
      "Average loss at step 727999:   3.3\n",
      "Average loss at step 729999:   3.1\n",
      "Average loss at step 731999:   3.2\n",
      "Average loss at step 733999:   3.3\n",
      "Average loss at step 735999:   2.8\n",
      "Average loss at step 737999:   3.1\n",
      "Average loss at step 739999:   3.2\n",
      "Average loss at step 741999:   3.2\n",
      "Average loss at step 743999:   3.3\n",
      "Average loss at step 745999:   3.1\n",
      "Average loss at step 747999:   3.2\n",
      "Average loss at step 749999:   3.0\n",
      "Average loss at step 751999:   2.8\n",
      "Average loss at step 753999:   3.3\n",
      "Average loss at step 755999:   3.2\n",
      "Average loss at step 757999:   3.4\n",
      "Average loss at step 759999:   3.3\n",
      "Average loss at step 761999:   3.0\n",
      "Average loss at step 763999:   3.3\n",
      "Average loss at step 765999:   2.9\n",
      "Average loss at step 767999:   2.7\n",
      "Average loss at step 769999:   3.0\n",
      "Average loss at step 771999:   3.0\n",
      "Average loss at step 773999:   3.3\n",
      "Average loss at step 775999:   2.8\n",
      "Average loss at step 777999:   3.1\n",
      "Average loss at step 779999:   3.2\n",
      "Average loss at step 781999:   3.4\n",
      "Average loss at step 783999:   2.9\n",
      "Average loss at step 785999:   3.3\n",
      "Average loss at step 787999:   3.3\n",
      "Average loss at step 789999:   3.1\n",
      "Average loss at step 791999:   3.4\n",
      "Average loss at step 793999:   3.3\n",
      "Average loss at step 795999:   3.2\n",
      "Average loss at step 797999:   2.8\n",
      "Average loss at step 799999:   3.0\n",
      "Average loss at step 801999:   3.3\n",
      "Average loss at step 803999:   3.4\n",
      "Average loss at step 805999:   3.3\n",
      "Average loss at step 807999:   3.3\n",
      "Average loss at step 809999:   3.0\n",
      "Average loss at step 811999:   2.7\n",
      "Average loss at step 813999:   2.7\n",
      "Average loss at step 815999:   2.5\n",
      "Average loss at step 817999:   2.5\n",
      "Average loss at step 819999:   2.9\n",
      "Average loss at step 821999:   2.5\n",
      "Average loss at step 823999:   2.4\n",
      "Average loss at step 825999:   2.2\n",
      "Average loss at step 827999:   2.2\n",
      "Average loss at step 829999:   3.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3b9b84276a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_embed_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_TRAIN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSKIP_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-4c2ad49fd924>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, batch_gen, num_train_steps, learning_rate, skip_step)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op],\n\u001b[0;32m---> 21\u001b[0;31m                                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_embed_matrix = train_model(model, batch_gen, NUM_TRAIN_STEPS, LEARNING_RATE, SKIP_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, learning_rate=400.0, perplexity=40)\n",
    "XX = tsne.fit_transform(final_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(XX, columns=['x0', 'x1'])\n",
    "unique_codones = sorted(dictionary, key=dictionary.get)\n",
    "tsne_df['codone'] = list(unique_codones)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tsne_df(df):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('unlabeled encoding', fontsize=20)\n",
    "    plt.scatter(df.x0, df.x1, s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_df(tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/acid_properties.csv'\n",
    "props = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acid_dict(some_c, props):\n",
    "    prop_by_letter = [props[props.acid == let].iloc[:, 1:] for let in some_c]   \n",
    "    df_concat = pd.concat(prop_by_letter)\n",
    "    res = df_concat.mean()\n",
    "    dres = dict(res)\n",
    "    dres['acid'] = some_c\n",
    "    return dres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/all_acid_dicts.pickle'\n",
    "producer = lambda: [acid_dict(some_c, props) for some_c in tsne_df.codone]\n",
    "all_acid_dicts = read_or_create(save_path, producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acid_df = pd.DataFrame(all_acid_dicts)\n",
    "all_acid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_acid_df.join(tsne_df.set_index('codone'), on='acid')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding_properties(final_df):\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    for i, p in enumerate(['hydrophobicity', 'mass', 'number_of_atoms', 'volume']):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.title(p, fontsize=25)\n",
    "        plt.scatter(final_df.x0, final_df.x1, c=final_df[p], s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/nice_embed_tsne.csv'\n",
    "gensim_tsne_df = pd.read_csv(filename, index_col=0)\n",
    "gensim_tsne_df.columns = ['x0', 'x1', 'codone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_df(gensim_tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df_nice = all_acid_df.join(gensim_tsne_df.set_index('codone'), on='acid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df_nice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Improve SkipGramModel to archive better embedding for amino acids codones. Visualize your space in the similar style as on the bottom example. You are only allowed to use vanilla tensorflow for this task.\n",
    "\n",
    "Article with the original research can be found here http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0141287&type=printable\n",
    "\n",
    "Bonus task(no credit): visualize your embedding space in similar manner as minst example: https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
